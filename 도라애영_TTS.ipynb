{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "도라애영_TTS",
      "provenance": [],
      "authorship_tag": "ABX9TyOxZ/TSBPFX/Icr71im9UOj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blackgreenc/AI/blob/main/%EB%8F%84%EB%9D%BC%EC%95%A0%EC%98%81_TTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqQ0owOIVk7c"
      },
      "source": [
        "# 도라애영씨 스트리밍 사이트 tts임."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxaQ44DQ6zyd"
      },
      "source": [
        "# Pre-net\n",
        "# Pre-training in AI refers to training a model with one task to help it form parameters that can be used in other tasks."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAOgo1np7x2F"
      },
      "source": [
        "<h2>Pre-training</h2>\n",
        "Pre-training in AI refers to training a model with one task to help it form parameters that can be used in other tasks.The concept of pre-training is inspired by human beings. Thanks to an innate ability, we don’t have to learn everything from scratch. Instead, we transfer and reuse our old knowledge of what we have learned in the past to understand new knowledge and handle a variety of new tasks.\n",
        "In AI, pre-training imitates the way human beings process new knowledge. That is: using model parameters of tasks that have been learned before to initialize the model parameters of new tasks. In this way, the old knowledge helps new models successfully perform new tasks from old experience instead of from scratch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T6YqnFq8n3t"
      },
      "source": [
        "Pre-training is bouilted by 5 methods.\n",
        "\n",
        "\n",
        "<h4>Word2vec</h4>\n",
        "Word2vec is a tool created by Google that produces static word embedding.\n",
        "<br><br>\n",
        "In 2013, Google open-sourced a famous tool for word embedding, word2vec, which can be efficiently trained on millions of words. It turns out that the word embedding trained by word2vec has the seemingly magical ability to measure word-to-word similarity.<br>\n",
        "So let’s take a quick look under the hood! <br>\n",
        "The word2vec uses is a shallow neural network with each word’s one-hot embedding as its input and output.<br><br>\n",
        "What does one-hot embedding look like?? <br>\n",
        "If a dictionary has four words, {‘have’, ‘a’, ‘good’, ‘day’}, the one-hot embedding of the word “good” is [0, 0, 1, 0].<br><br>\n",
        "One way of training is to predict a target word (the word’s one-hot embedding) as output, with the one-hot embedding of its surrounding words as input. Another way is to predict the surrounding words as output with a target word as input, which corresponds individually to CBOW and Skip-gram.\n",
        "\n",
        "\n"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "도라애영_TTS",
      "provenance": [],
      "authorship_tag": "ABX9TyMTJgzStCp8izMC8J+gwTLB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blackgreenc/AI/blob/main/%EB%8F%84%EB%9D%BC%EC%95%A0%EC%98%81_TTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqQ0owOIVk7c"
      },
      "source": [
        "# 도라애영씨 스트리밍 사이트 tts임."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxaQ44DQ6zyd"
      },
      "source": [
        "# Pre-net\n",
        "# Pre-training in AI refers to training a model with one task to help it form parameters that can be used in other tasks."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAOgo1np7x2F"
      },
      "source": [
        "<h2>Pre-training</h2>\n",
        "Pre-training in AI refers to training a model with one task to help it form parameters that can be used in other tasks.The concept of pre-training is inspired by human beings. Thanks to an innate ability, we don’t have to learn everything from scratch. Instead, we transfer and reuse our old knowledge of what we have learned in the past to understand new knowledge and handle a variety of new tasks.\n",
        "In AI, pre-training imitates the way human beings process new knowledge. That is: using model parameters of tasks that have been learned before to initialize the model parameters of new tasks. In this way, the old knowledge helps new models successfully perform new tasks from old experience instead of from scratch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T6YqnFq8n3t"
      },
      "source": [
        "Pre-training is bouilted by 5 methods.\n",
        "\n",
        "\n",
        "<h3>1, Word2vec</h3>\n",
        "Word2vec is a tool created by Google that produces static word embedding.\n",
        "<br><br>\n",
        "In 2013, Google open-sourced a famous tool for word embedding, word2vec, which can be efficiently trained on millions of words. It turns out that the word embedding trained by word2vec has the seemingly magical ability to measure word-to-word similarity.<br>\n",
        "So let’s take a quick look under the hood! <br>\n",
        "The word2vec uses is a shallow neural network with each word’s one-hot embedding as its input and output.<br><br>\n",
        "What does one-hot embedding look like?? <br>\n",
        "If a dictionary has four words, {‘have’, ‘a’, ‘good’, ‘day’}, the one-hot embedding of the word “good” is [0, 0, 1, 0].<br><br>\n",
        "One way of training is to predict a target word (the word’s one-hot embedding) as output, with the one-hot embedding of its surrounding words as input. Another way is to predict the surrounding words as output with a target word as input, which corresponds individually to CBOW and Skip-gram.<br><br>\n",
        "After the training is finished, the parameter matrix of the model will have formed the word-embedding dictionary — so we get each word’s embedding of the training data.<br><br>\n",
        "In fact, when we talk about the word2vec algorithm or model, it actually refers to the CBOW and Skip-gram models. Many people think that word2vec itself refers to an algorithm or model but this is a misunderstanding.\n",
        "<br><br>\n",
        "On a final note, Hierarchical Softmax and Negative Sampling are optimization algorithms for training the CBOW and Skip-gram models.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "<h3>2, ELMo</h3>\n",
        " ELMo is an AI technology for dynamic word embedding with LSTM. <br><br>\n",
        " To understand ELMo, we must consider that the word embedding learned from Word2Vec is static: no matter the context, it will not change. This leads to a situation where word2vec’s word embedding can’t handle polysemous words.\n",
        "<br><br>\n",
        "But we can adjust the word embedding according to the context to give it a more semantic meaning — which is where ELMo comes into play. It is the idea of ​​dynamically adjusting word embedding according to the current context.<br><br>\n",
        "ELMo’s network architecture uses a two-layer bidirectional LSTM on top of the word embedding layer learned by word2vec.<br><br>\n",
        "To understand how this works, let’s work up from the word2vec layer, the product of which is embedding1.<br><br>\n",
        "Next, we add two bidirectional LSTM layers, where each layer is composed of two LSTMs — one from left to right and the other one from right to left. This gives us a second and third layer on top of the first. If we take a closer look at layer2, as an example, we’ll see that the two LSTMs within it generate two new embeddings.<br><br>\n",
        "After concatenating the two embeddings, we have a new embedding2 of this layer, which we feed into the third layer. The bidirectional LSTM in layer3 generates embedding3 in the same way as in layer2. <br><br>\n",
        "Then all three embeddings are weighted and summed up. Now, a new word embedding is obtained with syntactic (embedding2, the second-layer output) and semantic (embedding3, the third-layer output) information coded in it.<br><br>\n",
        "ELMo’s pre-training process not only learns word embedding but also learns a two-layer, bi-directional LSTM network structure — both of which have their own roles when facing new tasks.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "<h3> 3, GPT </h3>\n",
        "In AI, GPT is a transformer-decoder-based autoregressive language model.<br><br>\n",
        "The core idea behind the Transformer model is self-attention — the ability to attend to different positions of the input sequence to compute a representation of that sequence. To understand the Transformer, we need to discuss the attention mechanism and self-attention mechanism — which we will in detail in future articles.<br><br>\n",
        "You can also refer to Jay Alammar’s blog post, The Illustrated Transformer, which gives a very clear explanation of the Transformer. But for now, we only need to understand that the Transformer consists of two parts: an encoder and a decoder.<br><br>\n",
        "GPT is a method based on the Transformer decoder.<b> GPT also uses static word embedding as input, plus a certain number of layers of the Transformer decoder on top of it.<b> <br><br>\n",
        "As an example, let’s take another four-word sentence: “w1, w2, w3, w4.”<br><br>\n",
        "Now let’s take w3 as the current word. After its embedding passes through a decoder layer, it will become a new embedding — which will include the attention information w3 paid to w1 and w2. Note that w1 and w2 are to the left of w3.<br><br>\n",
        "We will explain attention information in articles to come but for now, you can simply think of it as a new kind of embedding. This gives the new embedding a better representation to better complete the task of predicting the next word from left to right.\n",
        "<br><br>\n",
        "When fine-tuning specific NLP tasks with supervised learning, GPT — unlike ELMo, which connects to other model layers as feature representations — does not need to re-build new model structures for tasks. Instead, it directly connects the last layer to softmax as the task output layer, then fine-tunes the entire model.<br><br>\n",
        "When fine-tuning specific NLP tasks with supervised learning, GPT — unlike ELMo, which connects to other model layers as feature representations — does not need to re-build new model structures for tasks. Instead, it directly connects the last layer to softmax as the task output layer, then fine-tunes the entire model.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "<h3> 4, BERT </h3>\n",
        "In AI, BERT is a Transformer-encoder-based autoencoder language model.<br><br>\n",
        "In the paper <u>“BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,”</u> Google divided pre-trained language representation methods into feature-based methods (ELMo) and fine-tuning-based methods (GPT).<br><br>\n",
        "BERT is a fine-tuning-based and encoder-based method (where you wil recall that GPT is decoder-based).<br><br>\n",
        "BERT changes the unidirectional language model in GPT into a bidirectional one. Instead of using the standard left-to-right prediction of the next word as the target task, BERT proposes two new tasks.<br><br>\n",
        "BERT’s first pre-training task is called MLM, or Masked Language Model. In the input word sequence of this model, 15% of the words are randomly masked and the task is to predict what they are. What we see is that, unlike previous models, BERT can predict these words from both directions — not just left-to-right or right-to-left.<br><br>\n",
        "In addition, in terms of model input, BERT WordPiece embedding is used instead of standard word embedding. That’s because BERT, by default, doesn’t use words as tokens but word pieces. For example, ‘playing’ is ‘play’, ‘ing’ instead of ‘playing’. From here, a position embedding and a segment embedding are added.\n",
        "<br><br>\n",
        "The position embedding alleviates the shortcomings of self-attention in ignoring word position information. The segmentation embedding represents the division of two sentences. The two input sentences are represented by 0 and 1 in the segment embedding.<br><br>\n",
        "After summing up the WordPiece embedding, position embedding, and segment embedding, we get a new embedding with word information and position information. Then we input this embedding to the BERT Transformer encoder layer to start the training.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}